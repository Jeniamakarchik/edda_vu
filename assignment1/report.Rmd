---
title: "Assignment 1"
author: "Alexia Salomons, Nathan Maxwell Jones, Yauheniya Makarevich, group 71"
date: "27 February 2023"
output: pdf_document
fontsize: 11pt
highlight: tango
---

```{r, echo=FALSE}
options(digits=3)
```


## Exercise 1. 

```{r, include=FALSE}
birthweight <- readLines("data/birthweight.txt")
birthweight <- as.double(birthweight[2:length(birthweight)])
birthweight

# print(paste("Amount of observations: ", length(birthweight)))
```

```{r}
birthweight_mean <- mean(birthweight)
print(paste("Mean of the sample: ", birthweight_mean))
```

**a)** 

```{r}
par(mfrow=c(1,2))
qqnorm(y = birthweight)
hist(birthweight)
```
Given the QQ-plot and the histogram above, the data appears to be normal.

To calculate the CI-96% in R:

```{r}
t.test(birthweight, conf.level = 0.96)
```
With a 96% CI, we get [2808-3019]. In order to decrease this range to 100, we can reverse the calculations in order to check the required sample size. In order to get CI in the first place, we use $$2913 +/- 2.05(ste)$$, given either of the CI values, we can calculate the ste. For example, using the upper bound, we gather that $$3019 = 2913 + 2.05(ste)$$, resulting in $$ste =  51.7$$. In order to ensure a range of max 100, the $$ste \times 2.05$$ should not be higher than 50, which gives us a maximum $$ste = 24.39$$. 

The ste is given by $$ste = \frac{s}{\sqrt(n)}$$, this way we can calculate *s* with the data already found, $$51.7 = \frac{s}{\sqrt(188)}$$, which gives us $$s=708.87$$. In order to mazimixe ste at 24.39, we can plug this into the ste formula to find the new *n*, such that, $$24.39 = \frac{708.87}{\sqrt(n)}$$, giving us $$n=844.71$$. Therefore, a sample size of approximately 845 babies would be needed in order to have a 96% CI with a range of 100.

```{r}
B <- 1000
alpha <- 0.04
T_star <- numeric(B)

for(i in 1:B) {
  X_star <- sample(birthweight, replace = TRUE)
  T_star[i] <- mean(X_star)
}

T_star_q2 <- quantile(T_star, alpha/2)
T_star_q98 <- quantile(T_star, 1 - alpha/2)

c(2*birthweight_mean - T_star_q98, 2*birthweight_mean - T_star_q2)
```
```{r}
sum(T_star<T_star_q2)
```

<!-- #TODO: FIGURE OUT SAMPLE SIZE FOR HAVING CI INTERVAL LENGTH + 100 -->
<!-- 828 babies to get the CI -->

**b)**  An expert claims that the mean birthweight is bigger than 2800 gram. Verify this claim by using a relevant t-test, explain the meaning of the CI in the R-output for this test. Also propose and perform a suitable sign tests for this problem.

```{r}
t.test(birthweight, alternative = "greater", mu=2800)
```
We reject H0(p=0.01337), so H1 is true and mean of the sample is bigger than 2800. 
CI is infinite on right side, since the test is one-sided.


Binom test

H0: mean <= 2800,
H1: mean > 2800.

```{r}
greater_weight <- as.integer(birthweight > 2800)
binom.test(sum(greater_weight), length(greater_weight), p=0.5, alt="g")
```
We reject H0(p=0.03868), so H1 is true and mean of the sample is bigger than 2800. 

Both test confirmed the hypothesis that mean of the sample is bigger than 2800.

**c)** Propose a way to compute the powers of the t-test and sing test from b) at some $\mu$ > 2800, comment.
```{r}


```

**d)**  Let $p$ be the probability that birthweight of a newborn baby is less than 2600 gram. Using asymptotic normality, the expert computed the left end $\hat{p}=0.25$ of the confidence interval $[\hat{p_l}, \hat{p_r}]$ for $p$. Recover the whole confidence interval and its confidence level.
```{r}

```

**e)** The expert also reports that there were 34 male and 28 female babies among 62 who weighted less than 2600 gram, and 61 male and 65 female babies among the remaining 126 babies. The expert claims that the mean weight is different for male and female babies. Verify this claim by an appropriate test.


success: w > 2600
```{r}
prop.test(c(61, 65), c(95, 93))
```
We accept H0: p1-p2=0, where p1, p2 - proportions of the success in population.

## Exercise 2
A study tested whether cholesterol was reduced after using a certain brand of margarine as part of a low fat low cholesterol diet. The data set cholesterol.txt contains information on 18 people using margarine to reduce cholesterol: columns Before and After8weeks contain the cholesterol level (mmol/L) respectively before the diet and after 8 weeks on the diet.

```{r}
df <- as.data.frame(read.table("data/cholesterol.txt", header=TRUE))
head(df)
```


**a)** Make some relevant plots of this data set, comment on normality. Are there any inconsistencies in the data? Investigate whether the columns Before and After8weeks are correlated.
```{r}
diffs <- df[, 1] - df[, 2]
diffs
qqnorm(diffs)
```

```{r}
hist(diffs)
```
```{r}
shapiro.test(diffs)
```
Differences are normally distributed.

```{r}
shapiro.test(df[, 1])
shapiro.test(df[, 2])
```
```{r}
cor.test(df[, 1], df[, 2], method="pearson")
```

```{r}
# Create a first line
plot(1:length(df[, 1]), df[, 1], type = "b", pch=19, col = "red", xlab = "individual", ylab = "cholesterol")
lines(1:length(df[, 2]), df[, 2], pch=18, col = "blue", type = "b", lty=2)
legend("topleft", legend=c("Before", "After"), col=c("red", "blue"), lty = 1:2, cex=0.8)
```

**b)** Apply two relevant tests (cf. Lectures 2, 3) to verify whether the diet with low fat margarine has an effect (argue whether the data are paired or not). Is a permutation test applicable?

Data is paired since it is two different measurements of the same person and two samples are correlated.
Relevant test:

1. T-test paired test
```{r}
t.test(df[, 1], df[, 2], paired=2)
```
-> based on p-value we reject null-hypothesis that samples have the same mean. there is a difference between these two samples.

2. Permutation test - it is applicable because we are only testing for a difference between mean, not how they relate to each other.
```{r}
diff_mean <- function(x, y) {
  return(mean(x-y))
}

# original dataframe
stats <- diff_mean(df[, 1], df[, 2])
stats

B <- 1000
t_star <- numeric(B)

for (i in 1:B) {
  diff_star <- t(apply(cbind(df[, 1], df[, 2]), 1, sample))
  t_star[i] <- diff_mean(diff_star[, 1], diff_star[, 2])
}

hist(t_star)
# plot(rep(stats, 2), c(0, 50), col="red", lwd=2)
lines(rep(stats, 2), c(0, 50), col="red", lwd=2)
```
```{r}
# calculating p-value
pl <- sum(t_star < stats) / B
pr <- sum(t_star > stats) / B

p <- 2*min(pl, pr)
p
```

**c)**
$\mu = (\theta + 3) / 2, \theta > 3$
```{r}

mu <- mean(df[, 2])
mu # bar{x}

s <- sd(df[, 2])
s

theta_hat <- 2*mu - 3
theta_hat

alpha <- 0.05

n <- length(df[, 2])
t_alpha <- qt(1 - alpha/2, df=n)
t_alpha

theta_l <- theta_hat - t_alpha*s/sqrt(n)
theta_r <- theta_hat + t_alpha*s/sqrt(n)
c("[", theta_l,",", theta_r, "]")
```
We can improve the CI by having more individuals in the samples. We can improve since we know the distribution and we know the estimation for theta.

**d)** 

```{r}
t <- max(df[, 2])
t

n <- length(df[, 2])
n

for (theta in 3:12) {
  B <- 1000
  t_star <- numeric(B)
  
  for (i in 1:B) {
    x_star <- runif(n, min = 3, max = theta)
    t_star[i] <- max(x_star)
  }
  
  pl <- sum(t_star < t)/B
  pr <- sum(t_star > t)/B
  
  p <- 2* min(pl, pr)
  print(paste("Theta =", theta, ", ", p))
}
```

**e)**
Medium and proportion tests
```{r}
less_chol <- as.integer(df[, 2] < 6)
binom.test(sum(less_chol), length(less_chol), p=0.5, alt="g")
```
```{r}
less_chol <- as.integer(df[, 2] < 4.5)
binom.test(sum(less_chol),length(less_chol),p=0.25, alt="l")
```
## Exercise 3

```{r}
df <- as.data.frame(read.table("data/diet.txt", header=TRUE))
df["weight.lost"] <- df["preweight"] - df["weight6weeks"]
head(df)
```
**a)**
```{r}
# Create a first line
plot(1:length(df[,5]), df[,5], type = "b", pch=19, col = "red", xlab = "individual", ylab = "weight (kg)")
lines(1:length(df[,7]), df[,7], pch=18, col = "blue", type = "b", lty=2)
legend("topleft", legend=c("Before", "After"), col=c("red", "blue"), lty = 1:2, cex=0.8)
```
```{r}
t.test(df[,5], df[,7], paired=TRUE)
```
Therefore diet does have an effect on weight loss.

Now check assumptions (normality)
```{r}
hist(df[,8])
```
```{r}
qqnorm(df[,8])
```
```{r}
shapiro.test(df[,8])
```
Data is normal :)

**b)**
Fromat data
```{r}
df$diet <- as.factor(df$diet)
dietaov=lm(weight.lost~diet,data=df)
anova(dietaov)
summary(dietaov)
```
The best diet is number 3. 
```{r}
# Checking assumptions
qqnorm(residuals(dietaov))
plot(fitted(dietaov), residuals(dietaov))
```

```{r}
kruskal.test(df$weight.lost, df$diet)
```
This supports the ANOVA result.

**c)**

```{r}
df$gender <- as.factor(df$gender)
dietgenderaov <- lm(weight.lost~gender*diet,data=df)
anova(dietgenderaov)
```

```{r}
summary(dietgenderaov)
```

```{r}
par(mfrow=c(1, 2))
interaction.plot(df$gender, df$diet, df$weight.lost)
interaction.plot(df$diet, df$gender, df$weight.lost)
```

Assumption: diet depends on gender, we can see that effect of diet varies inside women.

```{r}
genderaov <- lm(weight.lost~gender,data=df)
anova(genderaov)
summary(genderaov)
```

```{r}
# Checking assumptions
par(mfrow=c(1, 2))
qqnorm(residuals(dietgenderaov))
plot(fitted(dietgenderaov), residuals(dietgenderaov))
```
Residuals do not look like something that is normally distributed, which violates our assumptions about normality and makes 

**e)**
We prefer b) as c) looks irrelevant for the weight loss. 
```{r}
print(paste('Diet 1:', dietaov$coefficients[1]))
print(paste('Diet 2:', dietaov$coefficients[1] + dietaov$coefficients[2]))
print(paste('Diet 3:', dietaov$coefficients[1] + dietaov$coefficients[3]))
```

## Exercise 4

**a)**
```{r, include=FALSE}
require("MASS")
```



```{r}
B <- 6; P <- 4; T <- 3

process <- c()
for (i in 1:B) {
  block <- c()
  for (tr in 1:T) {
    block <- cbind(block, as.numeric(sample(1:P) > 2))
  }
  process <- rbind(process, block)
}

process <- t(process)
rownames(process) <- c("N", "P", "K")
colnames(process) <- paste0(rep(1:6, each=4), paste0(".", rep(1:4, 6)))
process
```
In the table rows represent every soil additive and columns represent 6 blocks, each with 4 plots (first number - block, second number - plot). As you can see, every additive appears twice in each block.

**b)**
```{r}
interaction.plot(npk$block, npk$N, npk$yield, ylab='average yield', xlab='block', main='Average yield per block', legend=FALSE)
legend("topright", c("yes", "no"), title="N presence", lwd=1, lty=c(1, 2))
```

We have reason to believe that *block* may affect *yield*. This could happen because of slightly different environmental conditions: sun exposure, soil composition, etc. The plot supports this idea since it appears that average yield varies depending on the block when *N* is both present and absent.

**c)**
```{r}
n_block_lm <- lm(yield~block*N, data=npk)
anova(n_block_lm)
```
It is seen from the results that *N* and *block* are significant, but their interaction is not. Let's check summary for every level to see how each block individually affect the *yield*.

```{r}
summary(n_block_lm)
```

We can see that the difference between blocks is not significant except for block 3. Additionally, it seen that presence of *N* in the soil makes significant difference to the *yield*. Moreover, interaction between *block* and *N* is insignificant.

Because interaction is not significant in our case but *block* is significant by itself, we should go for the additive model.

```{r}
lm <- lm(yield~block+N, data=npk)
anova(lm)
```
Using the additive model we see that both *block* and *N* are still significant. Finally, we are going to check the model assumptions about normality of residuals.


```{r, echo=FALSE}
par(mfrow=c(1, 2))
qqnorm(residuals(lm))
plot(fitted(lm), residuals(lm))
```
The residuals and the Q-Q plot appear fairly normal.

Lastly, we cannot use Friedman test as we have two observations for each combination of soil additives.

**d)**
```{r}
npklm1 <- lm(yield~P + K + block*N, data=npk)
npklm2 <- lm(yield~N + K + block*P, data=npk)
npklm3 <- lm(yield~N + P + block*K, data=npk)
npklm4 <- lm(yield~block + N + P + K, data=npk)
npklm5 <- lm(yield~N + P + K, data=npk)
```

```{r}
print('Y ~ P + K + block*N')
anova(npklm1)
```
```{r}
print('Y ~ N + K + block*P')
anova(npklm2)
```
```{r}
print('Y ~ N + P + block*K')
anova(npklm3)
```

```{r}
print('Y ~ block + N + P + K')
anova(npklm4)
```

We have tested interaction models as well as additive. All the possible interactions between *block* and soil additives are insignificant. Therefore our preference goes to additive model as it shows significance of independent factors. Moreover, it is shown that *P* is an insignificant factor for the analysis.

```{r}
npklm5 <- lm(yield~block + N + K, data=npk)
anova(npklm5)
```
We believe that the model presented above is the best one, since all the factors are significant. But to examine further we have checked the normality for additive model with all the factors and for the additive model without *P*.

```{r, echo=FALSE}
par(mfrow=c(1, 2))
qqnorm(residuals(npklm4))
plot(fitted(npklm4), residuals(npklm4))
```

```{r, echo=FALSE}
par(mfrow=c(1, 2))
qqnorm(residuals(npklm5))
plot(fitted(npklm5), residuals(npklm5))
```

Since we cannot assume normality for the model without *P* factor, we are going to stick with the additive model which contains all the presented factors.


**e)**

```{r}
require(lme4)
npklmer <- lmer(yield~N+(1|block), REML=FALSE, data=npk)
npklmer1 <- lmer(yield~(1|block), REML=FALSE, data=npk)
anova(npklmer1, npklmer)
```

From the additional analysis can be seen that *N* has a significant effect on the *yield*, which supports what we found in the point **(c)**.
