---
title: "Assignment 1"
author: "Alexia Salomons, Nathan Maxwell Jones, Yauheniya Makarevich, group 71"
date: "27 February 2023"
output: pdf_document
fontsize: 11pt
highlight: tango
---

```{r, echo=FALSE}
options(digits=3)
```


## Exercise 1. 

```{r, include=FALSE}
birthweight <- readLines("data/birthweight.txt")
birthweight <- as.double(birthweight[2:length(birthweight)])
birthweight

# print(paste("Amount of observations: ", length(birthweight)))
```

```{r}
birthweight_mean <- mean(birthweight)
print(paste("Mean of the sample: ", birthweight_mean))
```

**a)** 

```{r}
par(mfrow=c(1,2))
qqnorm(y = birthweight)
hist(birthweight)
```
Given the QQ-plot and the histogram above, the data appears to be normal.

To calculate the CI-96% in R:

```{r}
t.test(birthweight, conf.level = 0.96)
```
With a 96% CI, we get [2808-3019]. In order to decrease this range to 100, we can reverse the calculations in order to check the required sample size. CI is given by $\left[\bar{X}-t_{\alpha / 2} \frac{s}{\sqrt{n}}, \bar{X}+t_{\alpha / 2} \frac{s}{\sqrt{n}}\right]$. Thus we can conclude that $t_{\alpha / 2} \frac{s}{\sqrt{n}} < 50$. From the data we can determine that $s=708.87$ and $t_{0.04 / 2} = 2.05$ (for 96% CI), and we can use this solve for $n = 844.71$.

Therefore, a sample size of approximately 845 babies would be needed in order to have a 96% CI with a range of 100.

```{r}
B <- 1000
alpha <- 0.04
T_star <- numeric(B)

for(i in 1:B) {
  X_star <- sample(birthweight, replace = TRUE)
  T_star[i] <- mean(X_star)
}

T_star_q2 <- quantile(T_star, alpha/2)
T_star_q98 <- quantile(T_star, 1 - alpha/2)

c(2*birthweight_mean - T_star_q98, 2*birthweight_mean - T_star_q2)
```
The CI in this case is very similar to the previous one, only changing the lower bound by 1.
```{r}
#sum(T_star<T_star_q2)
```


**b)**

$$H_0$$ = $$\mu =< 2800$$
$$H_1$$ = $$\mu > 2800$$

```{r}
t.test(birthweight, alternative = "greater", mu=2800)
```
As *p* is smaller than 0.05, we reject the null hypothesis, supporting the claim made by the expert. 
The CI is infinite on the right side, since the test is one-sided.

For a sign test, we can use:

```{r}
greater_weight <- as.integer(birthweight > 2800)
binom.test(sum(greater_weight), length(greater_weight), p=0.5, alt="g")
```
We again reject $$H_0$$, accepting $$H_1$$, that mean of the sample is bigger than 2800. 

Both test confirmed the hypothesis that the mean of the sample is bigger than 2800.

**c)** 
To compute the powers of both tests, we can assume that the data we have was sampled from a normal distribution with the same variance and $\mu = 2900$. We can then compute the probability of each test correctly rejecting $H_0: \mu \leq 2800$ as follows. To better approximate the current situation, we will choose a sample size of $n = 188$ newborn babies.
```{r}
# library(pwr)
#pwr.t.test(n=188,d=NULL,sig.level=0.05,type="one.sample",alternative="greater")

B=1000; n=188 ; mu = 2900; stdev = sd(birthweight);
psign=numeric(B)
pttest=numeric(B)
for(i in 1:B) {
  x=rnorm(n, mean=mu, sd=stdev)
  pttest[i]=t.test(x, alternative = "g", mu=2800)[[3]]
  psign[i]=binom.test(sum(x>2800), n, p=0.5)[[3]]
}
```

t-test power
```{r}
sum(pttest<0.05)/B
```
sign test power
```{r}
sum(psign<0.05)/B
```
We see that the power of the t-test is higher than that of the sign test. This makes sense because the sign test discards valuable information when considering only the signs, while the t-test is designed to test normal distributions, which is the case here.

**d)**  
```{r}

```

**e)** 


$$H_0$$ = $$P_(male) - P_(female) = 0$$
$$H_1$$ = $$P_(male) - P_(female) \neq 0$$
```{r}
prop.test(c(61, 65), c(95, 93))
```
As the p-value is bigger than 0.05, we fail to reject the null-hypothesis, meaning that there is no true difference between the mean weight of males and females. 

## Exercise 2

```{r, include=FALSE}
df <- as.data.frame(read.table("data/cholesterol.txt", header=TRUE))
head(df)
```

**a)**
```{r, results='hide'}
par(mfrow=c(2, 2))
hist(df[, 1], main="Before", xlab="cholesterol level")
hist(df[, 2], main="After", xlab="cholesterol level")
qqnorm(df[, 1])
qqnorm(df[, 2])
```

```{r}
shapiro.test(df[, 1])
shapiro.test(df[, 2])
```

As we can see from the plots and Shapiro test, the data is distributed normally.

# ```{r, results='hide'}
# diffs <- df[, 1] - df[, 2]
# par(mfrow=c(1, 2))
# qqnorm(diffs)
# hist(diffs)
# ```

# ```{r, echo=TRUE}
# shapiro.test(diffs)
# ```

```{r, echo=FALSE}
# Create a first line
plot(1:length(df[, 1]), df[, 1], type = "b", pch=19, col = "red", xlab = "individual", ylab = "cholesterol")
lines(1:length(df[, 2]), df[, 2], pch=18, col = "blue", type = "b", lty=2)
legend("topleft", legend=c("Before", "After"), col=c("red", "blue"), lty = 1:2, cex=0.8)
```

We draw the data as lines to see if there any consistences in data such as data incompleteness, outliers or meaningless data points.


```{r}
cor.test(df[, 1], df[, 2], method="pearson")
```
From the result of Pearson's test we can conclude that data before and data after are strongly correlated.

**b)**
Data is paired since it is  two different measurements of the same person and two samples are correlated.

In order to investigate has the diet affected individuals, we are going to apply tests (Paired t-test and Permutation test) that check difference between mean of different samples. Permutation test is applicable because it doesn't take normality into account and we are only testing for a difference between means, not how they relate to each other.

```{r}
t.test(df[, 1], df[, 2], paired=2)
```
Based on p-value obtained for paired T-test we reject null-hypothesis that samples have the same mean. Hence, there is a difference between these two samples.

```{r}
diff_mean <- function(x, y) {
  return(mean(x-y))
}

stats <- diff_mean(df[, 1], df[, 2])

B <- 1000
t_star <- numeric(B)

for (i in 1:B) {
  diff_star <- t(apply(cbind(df[, 1], df[, 2]), 1, sample))
  t_star[i] <- diff_mean(diff_star[, 1], diff_star[, 2])
}

hist(t_star, main="Histogram of statistics", xlab="values")
lines(rep(stats, 2), c(0, 50), col="red", lwd=2)
```
```{r}
# calculating p-value
pl <- sum(t_star < stats) / B
pr <- sum(t_star > stats) / B

p <- 2*min(pl, pr)
print(paste("P-value =", p))
```

We rejecting the null hypothesis that there is no difference between two samples.

**c)**
We have sample $x_i \in R[3, \theta]$, $i=\overline{1, 18}$, $\theta>3$. Analytic mean for this distribution will be $\mu = \frac{3 + \theta}{2}$.

We can estimate the mean and the standard distribution using our sample.
```{r, echo=FALSE}
mu <- mean(df[, 2])
s <- sd(df[, 2])
print(paste("mu =", mu, "s =", s))
```

From this value we can obtain the estimation for the parameter $\theta$. Knowing $\hat{\mu}$   $\hat{\theta} = 2*\hat{\mu} - 3$.
```{r, echo=FALSE}
theta_hat <- 2*mu - 3
print(paste("theta_hat =", theta_hat))
```

Then, using Central Limit Theorem, we can obtain confidence interval for parameter $\theta$:
$$[t_{-\frac{\alpha}{2}}\frac{s}{\sqrt{n}} + \frac{\theta + 3}{2}, t_{\frac{\alpha}{2}}\frac{s}{\sqrt{n}} + \frac{\theta + 3}{2}]$$

```{r}
alpha <- 0.05

n <- length(df[, 2])
t_alpha <- qt(1 - alpha/2, df=n)
theta_l <- theta_hat - t_alpha*s/sqrt(n)
theta_r <- theta_hat + t_alpha*s/sqrt(n)

print(paste0("95% confidence interval: [", theta_l,", ", theta_r, "]"))
```
We can improve the CI by having more individuals in the samples, as increasing in parameter *n* causes decreasing in standard deviation for sample of means. We can improve since we know the distribution and we know the estimation for theta.

**d)** 

```{r}
t <- max(df[, 2]); n <- length(df[, 2])

for (theta in 3:12) {
  B <- 1000
  t_star <- numeric(B)
  
  for (i in 1:B) {
    x_star <- runif(n, min = 3, max = theta)
    t_star[i] <- max(x_star)
  }
  
  pl <- sum(t_star < t)/B
  pr <- sum(t_star > t)/B
  
  p <- 2* min(pl, pr)
  print(paste("Theta =", theta, ", ", p, "KS = "))
}
```
Using bootstrap we have values of $\theta = 8$ and $\theta = 9$ for which our null hypothesis is not rejected (p-value > 0.05).
We can apply Kolmogorov-Smirnov test to examine the sample since it examine if two samples were drawn from the one distribution. We can generate sample from uniform distribution and apply test on them.

```{r}
ks.test(df[, 2], runif(100000, min = 3, max = 8))
```
P-value = 0.04, so we accept null hypothesis and can confirm that the sample was drown from  $U(3, \theta)$.

**e)**
We are using sign test to test if the median of the cholesterol level is $\l 6$.
```{r}
less_chol <- as.integer(df[, 2] < 6)
binom.test(sum(less_chol), length(less_chol), p=0.5, alt="g")
```



proportion tests
```{r}
less_chol <- as.integer(df[, 2] < 4.5)
binom.test(sum(less_chol),length(less_chol),p=0.25, alt="l")
```

## Exercise 3
We can compute and add the variable *weight.lost* as follows:
```{r}
df <- as.data.frame(read.table("data/diet.txt", header=TRUE))
df["weight.lost"] <- df["preweight"] - df["weight6weeks"]
head(df)
```
**a)**
To visualize the effect of diet on weight lost, we can plot the weights before and after the diets for every individual.
```{r}
plot(1:length(df[,5]), df[,5], type = "b", pch=19, col = "red", xlab = "individual", ylab = "weight (kg)")
lines(1:length(df[,7]), df[,7], pch=18, col = "blue", type = "b", lty=2)
legend("topleft", legend=c("Before", "After"), col=c("red", "blue"), lty = 1:2, cex=0.8)
```
We see the trend that the weights after 6 weeks of diet are lower for nearly all individuals. To test whether this trend is significant, we can apply a paired t-test as follows:
```{r}
t.test(df[,5], df[,7], paired=TRUE)
```
A p-value < 2.2e-16 indicates that diet does indeed have a significant effect on weight loss, with an estimated average loss of 3.845kg over the 6 week period.

To check the assumptions of the test, we need to verify that the difference between *preweight* and *weight6weeks* (ie. *weight.lost*) follows a normal distribution.
```{r}
par(mfrow=c(1, 2))
hist(df[,8])
qqnorm(df[,8])
```
```{r}
shapiro.test(df[,8])
```
Looking at the shape of the Q-Q plot and histogram as well as the result of the result of the Shapiro-Wilk normality test, we can conclude that *weight.lost* follows a normal distribution. Thus the test assumptions are valid.

**b)**
Format data
```{r}
df$diet <- as.factor(df$diet)
dietaov=lm(weight.lost~diet,data=df)
anova(dietaov)
summary(dietaov)
```
From the ANOVA table, we can see that p = 0.0032, meaning that the effect of diet on weight loss is significant. From the summary table, we can see that diet 2 is worse than diet 1, however, this difference is not significant. Furthermore, we can see that diet 3 is better than diet 1 and that this difference is significant with a p=0.0075. Therefore, diet 3 is the best diet. 
```{r}
par(mfrow=c(1,2))
qqnorm(residuals(dietaov))
plot(fitted(dietaov), residuals(dietaov))
```
The data seems to be relatively normal, therefore, it is appropriate to use ANOVA. Generally, the Kruskal-Wallis test is used when the data does not meet the assumptions for ANOVA, even though the data meets the assumptions here, the Kruskal-Wallis can still be used:

```{r}
kruskal.test(df$weight.lost, df$diet)
```
This supports the ANOVA result as the p-value is smaller than 0.05.

**c)**

```{r}
df$gender <- as.factor(df$gender)
dietgenderaov <- lm(weight.lost~gender*diet,data=df)
anova(dietgenderaov)
```
From this table we can see that gender does not have an effect of its own, however, it interacts with diet to affect weight loss.

```{r}
summary(dietgenderaov)
```

```{r}
par(mfrow=c(1, 2))
interaction.plot(df$gender, df$diet, df$weight.lost)
interaction.plot(df$diet, df$gender, df$weight.lost)
```

Assumption: diet depends on gender, we can see that effect of diet varies for women.

```{r}
genderaov <- lm(weight.lost~gender,data=df)
anova(genderaov)
summary(genderaov)
```
AS has also been shown in the previous ANOVA table, gender does not have an effect on weight loss on its own. It only has an effect when taking diet into account.
```{r}
# Checking assumptions
par(mfrow=c(1, 2))
qqnorm(residuals(dietgenderaov))
plot(fitted(dietgenderaov), residuals(dietgenderaov))
```
The residuals and QQ-plot of the gender and diet plot do not appear to be normally distributed, which violates our assumptions about normality. Therefore, one must take our previous results with caution. 

**d)** Skipped, as instructed.

**e)**
We prefer the model from *b* as *c* looks irrelevant for the weight loss and violates the needed assumptions. 
```{r}
print(paste('Diet 1:', dietaov$coefficients[1]))
print(paste('Diet 2:', dietaov$coefficients[1] + dietaov$coefficients[2]))
print(paste('Diet 3:', dietaov$coefficients[1] + dietaov$coefficients[3]))
```
An average person would lose approximately 3 kg for both diet 1 and 2, while losing approximately 5 kg for diet 3. 

## Exercise 4

**a)**
```{r, include=FALSE}
require("MASS")
```



```{r}
B <- 6; P <- 4; T <- 3

process <- c()
for (i in 1:B) {
  block <- c()
  for (tr in 1:T) {
    block <- cbind(block, as.numeric(sample(1:P) > 2))
  }
  process <- rbind(process, block)
}

process <- t(process)
rownames(process) <- c("N", "P", "K")
colnames(process) <- paste0(rep(1:6, each=4), paste0(".", rep(1:4, 6)))
process
```
In the table rows represent every soil additive and columns represent 6 blocks, each with 4 plots (first number - block, second number - plot). As you can see, every additive appears twice in each block.

**b)**
```{r}
interaction.plot(npk$block, npk$N, npk$yield, ylab='average yield', xlab='block', main='Average yield per block', legend=FALSE)
legend("topright", c("yes", "no"), title="N presence", lwd=1, lty=c(1, 2))
```

We have reason to believe that *block* may affect *yield*. This could happen because of slightly different environmental conditions: sun exposure, soil composition, etc. The plot supports this idea since it appears that average yield varies depending on the block when *N* is both present and absent.

**c)**
```{r}
n_block_lm <- lm(yield~block*N, data=npk)
anova(n_block_lm)
```
It is seen from the results that *N* and *block* are significant, but their interaction is not. Let's check summary for every level to see how each block individually affect the *yield*.

```{r}
summary(n_block_lm)
```

We can see that the difference between blocks is not significant except for block 3. Additionally, it seen that presence of *N* in the soil makes significant difference to the *yield*. Moreover, interaction between *block* and *N* is insignificant.

Because interaction is not significant in our case but *block* is significant by itself, we should go for the additive model.

```{r}
lm <- lm(yield~block+N, data=npk)
anova(lm)
```
Using the additive model we see that both *block* and *N* are still significant. Finally, we are going to check the model assumptions about normality of residuals.


```{r, echo=FALSE}
par(mfrow=c(1, 2))
qqnorm(residuals(lm))
plot(fitted(lm), residuals(lm))
```
The residuals and the Q-Q plot appear fairly normal.

Lastly, we cannot use Friedman test as we have two observations for each combination of soil additives.

**d)**
```{r}
npklm1 <- lm(yield~P + K + block*N, data=npk)
npklm2 <- lm(yield~N + K + block*P, data=npk)
npklm3 <- lm(yield~N + P + block*K, data=npk)
npklm4 <- lm(yield~block + N + P + K, data=npk)
npklm5 <- lm(yield~N + P + K, data=npk)
```

```{r}
print('Y ~ P + K + block*N')
anova(npklm1)
```
```{r}
print('Y ~ N + K + block*P')
anova(npklm2)
```
```{r}
print('Y ~ N + P + block*K')
anova(npklm3)
```

```{r}
print('Y ~ block + N + P + K')
anova(npklm4)
```

We have tested interaction models as well as additive. All the possible interactions between *block* and soil additives are insignificant. Therefore our preference goes to additive model as it shows significance of independent factors. Moreover, it is shown that *P* is an insignificant factor for the analysis.

```{r}
npklm5 <- lm(yield~block + N + K, data=npk)
anova(npklm5)
```
We believe that the model presented above is the best one, since all the factors are significant. But to examine further we have checked the normality for additive model with all the factors and for the additive model without *P*.

```{r, echo=FALSE}
par(mfrow=c(1, 2))
qqnorm(residuals(npklm4))
plot(fitted(npklm4), residuals(npklm4))
```

```{r, echo=FALSE}
par(mfrow=c(1, 2))
qqnorm(residuals(npklm5))
plot(fitted(npklm5), residuals(npklm5))
```

Since we cannot assume normality for the model without *P* factor, we are going to stick with the additive model which contains all the presented factors.


**e)**

```{r}
require(lme4)
npklmer <- lmer(yield~N+(1|block), REML=FALSE, data=npk)
npklmer1 <- lmer(yield~(1|block), REML=FALSE, data=npk)
anova(npklmer1, npklmer)
```

From the additional analysis can be seen that *N* has a significant effect on the *yield*, which supports what we found in the point **(c)**.
