---
title: "Assignment 1"
author: "Alexia Salomons, Nathan Maxwell Jones, Yauheniya Makarevich, group 71"
date: "27 February 2023"
output: pdf_document
fontsize: 11pt
highlight: tango
---

```{r, echo=FALSE}
options(digits=3)
```


## Exercise 1. 

```{r, include=FALSE}
birthweight <- readLines("data/birthweight.txt")
birthweight <- as.double(birthweight[2:length(birthweight)])
birthweight

# print(paste("Amount of observations: ", length(birthweight)))
```

```{r}
birthweight_mean <- mean(birthweight)
print(paste("Mean of the sample: ", birthweight_mean))
```

**a)** Check normality of the data. Assuming normality (irrespective of your conclusion about normality), construct a bounded 96%-CI for $\mu$. Evaluate the sample size needed to provide that the length of the 96%-CI is at most 100. Compute a bootstrap 96%-CI for $\mu$ and compare it to the above CI.


```{r}
qqnorm(y = birthweight)
```

```{r}
hist(birthweight)
# add density line
# lines(density(birthweight), col="blue",lwd=2)
```

```{r}
shapiro.test(birthweight)
# H0 - normal distribution, H1 - not normal
```
NORMAL DISTRIBUTION!

Let's go for CI-96%:

```{r}
t.test(birthweight, conf.level = 0.96)
```
```{r}
B <- 1000
alpha <- 0.04
T_star <- numeric(B)

for(i in 1:B) {
  X_star <- sample(birthweight, replace = TRUE)
  T_star[i] <- mean(X_star)
}

T_star_q2 <- quantile(T_star, alpha/2)
T_star_q98 <- quantile(T_star, 1 - alpha/2)

c(2*birthweight_mean - T_star_q98, 2*birthweight_mean - T_star_q2)
```
```{r}
sum(T_star<T_star_q2)
```

<!-- #TODO: FIGURE OUT SAMPLE SIZE FOR HAVING CI INTERVAL LENGTH + 100 -->
<!-- 828 babies to get the CI -->

**b)**  An expert claims that the mean birthweight is bigger than 2800 gram. Verify this claim by using a relevant t-test, explain the meaning of the CI in the R-output for this test. Also propose and perform a suitable sign tests for this problem.

```{r}
t.test(birthweight, alternative = "greater", mu=2800)
```
We reject H0(p=0.01337), so H1 is true and mean of the sample is bigger than 2800. 
CI is infinite on right side, since the test is one-sided.


Binom test

H0: mean <= 2800,
H1: mean > 2800.

```{r}
greater_weight <- as.integer(birthweight > 2800)
binom.test(sum(greater_weight), length(greater_weight), p=0.5, alt="g")
```
We reject H0(p=0.03868), so H1 is true and mean of the sample is bigger than 2800. 

Both test confirmed the hypothesis that mean of the sample is bigger than 2800.

**c)** Propose a way to compute the powers of the t-test and sing test from b) at some $\mu$ > 2800, comment.
```{r}


```

**d)**  Let $p$ be the probability that birthweight of a newborn baby is less than 2600 gram. Using asymptotic normality, the expert computed the left end $\hat{p}=0.25$ of the confidence interval $[\hat{p_l}, \hat{p_r}]$ for $p$. Recover the whole confidence interval and its confidence level.
```{r}

```

**e)** The expert also reports that there were 34 male and 28 female babies among 62 who weighted less than 2600 gram, and 61 male and 65 female babies among the remaining 126 babies. The expert claims that the mean weight is different for male and female babies. Verify this claim by an appropriate test.


success: w > 2600
```{r}
prop.test(c(61, 65), c(95, 93))
```
We accept H0: p1-p2=0, where p1, p2 - proportions of the success in population.

## Exercise 2

```{r, include=FALSE}
df <- as.data.frame(read.table("data/cholesterol.txt", header=TRUE))
head(df)
```

**a)**
```{r, results='hide'}
par(mfrow=c(2, 2))
hist(df[, 1], main="Before", xlab="cholesterol level")
hist(df[, 2], main="After", xlab="cholesterol level")
qqnorm(df[, 1])
qqnorm(df[, 2])
```

```{r}
shapiro.test(df[, 1])
shapiro.test(df[, 2])
```

As we can see from the plots and Shapiro test, the data is distributed normally.

# ```{r, results='hide'}
# diffs <- df[, 1] - df[, 2]
# par(mfrow=c(1, 2))
# qqnorm(diffs)
# hist(diffs)
# ```

# ```{r, echo=TRUE}
# shapiro.test(diffs)
# ```

```{r, echo=FALSE}
# Create a first line
plot(1:length(df[, 1]), df[, 1], type = "b", pch=19, col = "red", xlab = "individual", ylab = "cholesterol")
lines(1:length(df[, 2]), df[, 2], pch=18, col = "blue", type = "b", lty=2)
legend("topleft", legend=c("Before", "After"), col=c("red", "blue"), lty = 1:2, cex=0.8)
```

We draw the data as lines to see if there any consistences in data such as data incompleteness, outliers or meaningless data points.


```{r}
cor.test(df[, 1], df[, 2], method="pearson")
```
From the result of Pearson's test we can conclude that data before and data after are strongly correlated.

**b)**
Data is paired since it is  two different measurements of the same person and two samples are correlated.

In order to investigate has the diet affected individuals, we are going to apply tests (Paired t-test and Permutation test) that check difference between mean of different samples. Permutation test is applicable because it doesn't take normality into account and we are only testing for a difference between means, not how they relate to each other.

```{r}
t.test(df[, 1], df[, 2], paired=2)
```
Based on p-value obtained for paired T-test we reject null-hypothesis that samples have the same mean. Hence, there is a difference between these two samples.

```{r}
diff_mean <- function(x, y) {
  return(mean(x-y))
}

stats <- diff_mean(df[, 1], df[, 2])

B <- 1000
t_star <- numeric(B)

for (i in 1:B) {
  diff_star <- t(apply(cbind(df[, 1], df[, 2]), 1, sample))
  t_star[i] <- diff_mean(diff_star[, 1], diff_star[, 2])
}

hist(t_star, main="Histogram of statistics", xlab="values")
lines(rep(stats, 2), c(0, 50), col="red", lwd=2)
```
```{r}
# calculating p-value
pl <- sum(t_star < stats) / B
pr <- sum(t_star > stats) / B

p <- 2*min(pl, pr)
print(paste("P-value =", p))
```

We rejecting the null hypothesis that there is no difference between two samples.

**c)**
We have sample $x_i \in R[3, \theta]$, $i=\overline{1, 18}$, $\theta>3$. Analytic mean for this distribution will be $\mu = \frac{3 + \theta}{2}$.

We can estimate the mean and the standard distribution using our sample.
```{r, echo=FALSE}
mu <- mean(df[, 2])
s <- sd(df[, 2])
print(paste("mu =", mu, "s =", s))
```

From this value we can obtain the estimation for the parameter $\theta$. Knowing $\hat{\mu}$   $\hat{\theta} = 2*\hat{\mu} - 3$.
```{r, echo=FALSE}
theta_hat <- 2*mu - 3
print(paste("theta_hat =", theta_hat))
```

Then, using Central Limit Theorem, we can obtain confidence interval for parameter $\theta$:
$$[t_{-\frac{\alpha}{2}}\frac{s}{\sqrt{n}} + \frac{\theta + 3}{2}, t_{\frac{\alpha}{2}}\frac{s}{\sqrt{n}} + \frac{\theta + 3}{2}]$$

```{r}
alpha <- 0.05

n <- length(df[, 2])
t_alpha <- qt(1 - alpha/2, df=n)
theta_l <- theta_hat - t_alpha*s/sqrt(n)
theta_r <- theta_hat + t_alpha*s/sqrt(n)

print(paste0("95% confidence interval: [", theta_l,", ", theta_r, "]"))
```
We can improve the CI by having more individuals in the samples, as increasing in parameter *n* causes decreasing in standard deviation for sample of means. We can improve since we know the distribution and we know the estimation for theta.

**d)** 

```{r}
t <- max(df[, 2]); n <- length(df[, 2])

for (theta in 3:12) {
  B <- 1000
  t_star <- numeric(B)
  
  for (i in 1:B) {
    x_star <- runif(n, min = 3, max = theta)
    t_star[i] <- max(x_star)
  }
  
  pl <- sum(t_star < t)/B
  pr <- sum(t_star > t)/B
  
  p <- 2* min(pl, pr)
  print(paste("Theta =", theta, ", ", p, "KS = "))
}
```
Using bootstrap we have values of $\theta = 8$ and $\theta = 9$ for which our null hypothesis is not rejected (p-value > 0.05).
We can apply Kolmogorov-Smirnov test to examine the sample since it examine if two samples were drawn from the one distribution. We can generate sample from uniform distribution and apply test on them.

```{r}
ks.test(df[, 2], runif(100000, min = 3, max = 8))
```
P-value = 0.04, so we accept null hypothesis and can confirm that the sample was drown from  $U(3, \theta)$.

**e)**
We are using sign test to test if the median of the cholesterol level is $\l 6$.
```{r}
less_chol <- as.integer(df[, 2] < 6)
binom.test(sum(less_chol), length(less_chol), p=0.5, alt="g")
```



proportion tests
```{r}
less_chol <- as.integer(df[, 2] < 4.5)
binom.test(sum(less_chol),length(less_chol),p=0.25, alt="l")
```

## Exercise 3

```{r}
df <- as.data.frame(read.table("data/diet.txt", header=TRUE))
df["weight.lost"] <- df["preweight"] - df["weight6weeks"]
head(df)
```
**a)**
```{r}
# Create a first line
plot(1:length(df[,5]), df[,5], type = "b", pch=19, col = "red", xlab = "individual", ylab = "weight (kg)")
lines(1:length(df[,7]), df[,7], pch=18, col = "blue", type = "b", lty=2)
legend("topleft", legend=c("Before", "After"), col=c("red", "blue"), lty = 1:2, cex=0.8)
```
```{r}
t.test(df[,5], df[,7], paired=TRUE)
```
Therefore diet does have an effect on weight loss.

Now check assumptions (normality)
```{r}
hist(df[,8])
```
```{r}
qqnorm(df[,8])
```
```{r}
shapiro.test(df[,8])
```
Data is normal :)

**b)**
Fromat data
```{r}
df$diet <- as.factor(df$diet)
dietaov=lm(weight.lost~diet,data=df)
anova(dietaov)
summary(dietaov)
```
The best diet is number 3. 
```{r}
# Checking assumptions
qqnorm(residuals(dietaov))
plot(fitted(dietaov), residuals(dietaov))
```

```{r}
kruskal.test(df$weight.lost, df$diet)
```
This supports the ANOVA result.

**c)**

```{r}
df$gender <- as.factor(df$gender)
dietgenderaov <- lm(weight.lost~gender*diet,data=df)
anova(dietgenderaov)
```

```{r}
summary(dietgenderaov)
```

```{r}
par(mfrow=c(1, 2))
interaction.plot(df$gender, df$diet, df$weight.lost)
interaction.plot(df$diet, df$gender, df$weight.lost)
```

Assumption: diet depends on gender, we can see that effect of diet varies inside women.

```{r}
genderaov <- lm(weight.lost~gender,data=df)
anova(genderaov)
summary(genderaov)
```

```{r}
# Checking assumptions
par(mfrow=c(1, 2))
qqnorm(residuals(dietgenderaov))
plot(fitted(dietgenderaov), residuals(dietgenderaov))
```
Residuals do not look like something that is normally distributed, which violates our assumptions about normality and makes 

**e)**
We prefer b) as c) looks irrelevant for the weight loss. 
```{r}
print(paste('Diet 1:', dietaov$coefficients[1]))
print(paste('Diet 2:', dietaov$coefficients[1] + dietaov$coefficients[2]))
print(paste('Diet 3:', dietaov$coefficients[1] + dietaov$coefficients[3]))
```

## Exercise 4

**a)**
```{r, include=FALSE}
require("MASS")
```



```{r}
B <- 6; P <- 4; T <- 3

process <- c()
for (i in 1:B) {
  block <- c()
  for (tr in 1:T) {
    block <- cbind(block, as.numeric(sample(1:P) > 2))
  }
  process <- rbind(process, block)
}

process <- t(process)
rownames(process) <- c("N", "P", "K")
colnames(process) <- paste0(rep(1:6, each=4), paste0(".", rep(1:4, 6)))
process
```
In the table rows represent every soil additive and columns represent 6 blocks, each with 4 plots (first number - block, second number - plot). As you can see, every additive appears twice in each block.

**b)**
```{r}
interaction.plot(npk$block, npk$N, npk$yield, ylab='average yield', xlab='block', main='Average yield per block', legend=FALSE)
legend("topright", c("yes", "no"), title="N presence", lwd=1, lty=c(1, 2))
```

We have reason to believe that *block* may affect *yield*. This could happen because of slightly different environmental conditions: sun exposure, soil composition, etc. The plot supports this idea since it appears that average yield varies depending on the block when *N* is both present and absent.

**c)**
```{r}
n_block_lm <- lm(yield~block*N, data=npk)
anova(n_block_lm)
```
It is seen from the results that *N* and *block* are significant, but their interaction is not. Let's check summary for every level to see how each block individually affect the *yield*.

```{r}
summary(n_block_lm)
```

We can see that the difference between blocks is not significant except for block 3. Additionally, it seen that presence of *N* in the soil makes significant difference to the *yield*. Moreover, interaction between *block* and *N* is insignificant.

Because interaction is not significant in our case but *block* is significant by itself, we should go for the additive model.

```{r}
lm <- lm(yield~block+N, data=npk)
anova(lm)
```
Using the additive model we see that both *block* and *N* are still significant. Finally, we are going to check the model assumptions about normality of residuals.


```{r, echo=FALSE}
par(mfrow=c(1, 2))
qqnorm(residuals(lm))
plot(fitted(lm), residuals(lm))
```
The residuals and the Q-Q plot appear fairly normal.

Lastly, we cannot use Friedman test as we have two observations for each combination of soil additives.

**d)**
```{r}
npklm1 <- lm(yield~P + K + block*N, data=npk)
npklm2 <- lm(yield~N + K + block*P, data=npk)
npklm3 <- lm(yield~N + P + block*K, data=npk)
npklm4 <- lm(yield~block + N + P + K, data=npk)
npklm5 <- lm(yield~N + P + K, data=npk)
```

```{r}
print('Y ~ P + K + block*N')
anova(npklm1)
```
```{r}
print('Y ~ N + K + block*P')
anova(npklm2)
```
```{r}
print('Y ~ N + P + block*K')
anova(npklm3)
```

```{r}
print('Y ~ block + N + P + K')
anova(npklm4)
```

We have tested interaction models as well as additive. All the possible interactions between *block* and soil additives are insignificant. Therefore our preference goes to additive model as it shows significance of independent factors. Moreover, it is shown that *P* is an insignificant factor for the analysis.

```{r}
npklm5 <- lm(yield~block + N + K, data=npk)
anova(npklm5)
```
We believe that the model presented above is the best one, since all the factors are significant. But to examine further we have checked the normality for additive model with all the factors and for the additive model without *P*.

```{r, echo=FALSE}
par(mfrow=c(1, 2))
qqnorm(residuals(npklm4))
plot(fitted(npklm4), residuals(npklm4))
```

```{r, echo=FALSE}
par(mfrow=c(1, 2))
qqnorm(residuals(npklm5))
plot(fitted(npklm5), residuals(npklm5))
```

Since we cannot assume normality for the model without *P* factor, we are going to stick with the additive model which contains all the presented factors.


**e)**

```{r}
require(lme4)
npklmer <- lmer(yield~N+(1|block), REML=FALSE, data=npk)
npklmer1 <- lmer(yield~(1|block), REML=FALSE, data=npk)
anova(npklmer1, npklmer)
```

From the additional analysis can be seen that *N* has a significant effect on the *yield*, which supports what we found in the point **(c)**.
