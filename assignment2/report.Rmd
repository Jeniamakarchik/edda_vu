---
title: "Assignment 2"
author: "Alexia Salomons, Nathan Maxwell Jones, Yauheniya Makarevich, group 71"
date: "15 March 2023"
output: pdf_document
fontsize: 11pt
highlight: tango
header-includes: \usepackage{xcolor}
---

```{r setup, include = FALSE}
# set up global R options
options(digits = 3)

# set up knitr global chunk options
knitr::opts_chunk$set(fig.height = 3)
```


## Exercise 1

```{r, include=FALSE}
tree_df <- as.data.frame(read.table("data/treeVolume.txt", header=TRUE))
head(tree_df)
```

**a)**  
  
To investigate whether tree type influences total wood volume, we can perform a one-way ANOVA.

```{r}
tree_df$type <- as.factor(tree_df$type)
tree_type_lm <- lm(volume~type, data=tree_df)
anova(tree_type_lm)
```

With $p > 0.05$, we can conclude that *type* does not have a significant effect on *volume*. Because the factor *type* has two levels, we can apply an unpaired two sample t-test.

```{r}
mask <- tree_df$type == "beech"
t.test(tree_df$volume[mask], tree_df$volume[!mask])
```

Again we see that *type* is not significant. This supports the result from the ANOVA test. The estimated volume is 30.2 for Beech trees and 35.2 for Oak trees.

**b)**  
  
To investigate this claim, we create two models, each including all three explanatory variables (*type*, *diameter* and *height*). In the first model, we also include the pairwise interaction between *type* and *diameter*.

```{r}
tree_type_d_lm <- lm(volume~height+type*diameter, data=tree_df)
anova(tree_type_d_lm)
```

In the second model, we include the pairwise interaction between *type* and *height*.

```{r}
tree_type_h_lm <- lm(volume~diameter+type*height, data=tree_df)
anova(tree_type_h_lm)
```

We see that both pairwise interactions are not significant. Therefore, we can conclude that both *height* and *diameter* have the same influence on *volume* regardless of *type*.

**c)**  
  
In (b), we saw that the interactions of *height* and *diameter* with *type* were not significant, and so we will investigate a purely additive model (assuming no interactions).

```{r}
tree_add_all_lm <- lm(volume~diameter+height+type, data=tree_df)
drop1(tree_add_all_lm, test= "F")
```

We see that the effect of *type* is not significant in the additive model. Therefore we will investigate an additive model that excludes *type*.

```{r}
tree_add_dh_lm <- lm(volume~diameter+height, data=tree_df)
anova(tree_add_dh_lm)
```
```{r}
summary(tree_add_dh_lm)
```

This model has a high R-squared value while using fewer variables, all of which are significant. Since simpler models are generally preferred, this is our model of choice to make predictions. As a final test, we need to check this model's assumptions to ensure that the conclusions we draw from it are valid:

```{r, echo=FALSE}
par(mfrow=c(1, 2))
qqnorm(residuals(tree_add_dh_lm))
plot(fitted(tree_add_dh_lm), residuals(tree_add_dh_lm))
```

While these plots are not perfect, we believe the model assumptions to be valid. 

Therefore, the effects of *type*, *diameter* and *height* can be summarized as follows:

* The tree *type* does not affect volume significantly.
* Looking at the coefficients, we see that increasing both height and diameter result in an increase in volume, with diameter having a bigger impact (with a gradient of 4.63 compared to *height's* 0.43). This makes sense given that we know volume is proportional to the square of the diameter.

To predict the volume for a tree with the overall average diameter and height, we can use the following linear regression model:

$$volume = -64.37 + 4.63 * diameter + 0.43 * height$$

```{r}
mean_d <- mean(tree_df$diameter)
mean_h <- mean(tree_df$height)
means <-  data.frame(diameter=c(mean_d), height=c(mean_h))

predict(tree_add_dh_lm, means, interval = "confidence")
```

Therefore we expect the volume for such a tree to be 32.6, with a 95% CI of [31.7, 33.4].

**d)**  
  
Assuming that a tree is roughly cylindrical, we expect that *volume* would be proportional to the *height* multiplied by the square of *diameter*. We perform this transformation and add it as a new column in the data frame. We could apply the true transformation, $V = h \times \pi (d/2)^2$, but this would just add unnecessary constants which would already be captured in the regression coefficients. We also will not include *type* because it was not significant.

```{r}
tree_df$math_volume <- tree_df$height * tree_df$diameter^2
math_volume_lm <- lm(volume~math_volume, data=tree_df)
anova(math_volume_lm)
```

```{r}
summary(math_volume_lm)
```

We see that this transformation does indeed produce an explanatory value with a significant effect. We also see that the R-squared (0.975) and adjusted R-squared (0.974) values are higher than that of the model chosen in (c) (tree_add_dh_lm), indicating that it better explains the data. Finally, we check the assumptions of this model.

```{r, echo=FALSE}
par(mfrow=c(1,2))

qqnorm(residuals(math_volume_lm))
plot(fitted(math_volume_lm), residuals(math_volume_lm))
```

These plots are acceptable, meaning we can accept the model assumptions.

## Exercise 2

```{r, include=FALSE}
# Read data
crime_df <- as.data.frame(read.table("data/expensescrime.txt", header=TRUE))
head(crime_df)

response <- "expend"
exp_vars <- c("bad", "crime", "lawyers", "employ", "pop")
```

**a)** 

To investigate the interactions between all the variables of interest, we can plot the pairwise scatter plots for all their combinations:

```{r, echo=FALSE, fig.height=6}
pairs(crime_df[, c(response, exp_vars)])
```

We see that *expend*, our response variable, appears to have a positive correlation with all the explanatory variables except for *crime*. There appear to be several outliers at the high end of the data which could skew the model. We can also see that collinearity exists between the explanatory variables *bad*, *lawyers*, *employ* and *pop*. This is a problem since the redundant information will make the regression coefficients difficult to estimate. 

We can use Cook's distance to find the influence points (a distance greater than 1 indicates an outlier)

```{r}
crime_lm <- lm(expend~bad+crime+lawyers+employ+pop, data=crime_df)
cooks.distance(crime_lm)[cooks.distance(crime_lm) > 1]
```
```{r, include=FALSE}
plot(cooks.distance(crime_lm), type='b', ylab="Cook's distance", main = "Cook's distance for expensecrime.txt")
```

We can see that indices of 5, 8, 35 and 44 are outliers, which we can remove:

```{r}
crime_df_upd <- crime_df[-c(5,8,35,44),]
```

To further investigate collinearity, we can examine the correlations between all the explanatory variables, which confirms strong correlations between *bad*, *lawyers*, *employ* and *pop*.

```{r}
round(cor(crime_df[, c(exp_vars)]), 2)
```

We can also use the VIF to see which variables are collinear (VIF > 5 is cause for concern).

```{r, include=FALSE}
library(car)
```
```{r}
vif(lm(expend~bad+crime+lawyers+employ+pop, data=crime_df))
```

This further confirms that collinearity exists for the variables *bad*, *lawyers*, *employ* and *pop*.

**From this point on, we will proceed *without* the influence points.**

**b)** 

```{r, include=FALSE}
print("Step 1")

print("bad")
summary(lm(expend~bad, data=crime_df_upd))

print("crime")
summary(lm(expend~crime, data=crime_df_upd))

print("lawyers")
summary(lm(expend~lawyers, data=crime_df_upd))

print("employ")
summary(lm(expend~employ, data=crime_df_upd)) # YES

print("pop")
summary(lm(expend~pop, data=crime_df_upd)) 
```

```{r, include=FALSE}
print("Step 2")

print("bad")
summary(lm(expend~employ+bad, data=crime_df_upd))

print("crime")
summary(lm(expend~employ+crime, data=crime_df_upd)) # YES

print("lawyers")
summary(lm(expend~employ+lawyers, data=crime_df_upd))

print("pop")
summary(lm(expend~employ+pop, data=crime_df_upd))
```

```{r, include=FALSE}
print("Step 3")

print("bad")
summary(lm(expend~employ+crime+bad, data=crime_df_upd))

print("lawyers")
summary(lm(expend~employ+crime+lawyers, data=crime_df_upd))

print("pop")
summary(lm(expend~employ+crime+pop, data=crime_df_upd)) # YES
```

```{r, include=FALSE}
print("Step 4")

print("bad")
summary(lm(expend~employ+crime+pop+bad, data=crime_df_upd))

print("lawyers")
summary(lm(expend~employ+crime+pop+lawyers, data=crime_df_upd))

# None significant!
```

The step-up process was carried out. The variables added in order were *employ*, *crime* and *pop*, after which no further added variables had significant p-values. Hence the final model is as follows:

```{r}
step_up_lm <- lm(expend~employ+crime+pop, data=crime_df_upd)
summary(step_up_lm)
```

Final model: expend = -247 + 0.0209\*employ + 0.0543\*crime + 0.0714\*pop $\pm$ error, with $R^2 = 0.974$. Using VIF we can again check the model for collinearity.

```{r}
vif(step_up_lm)
```

We see that the step-up method naturally removes collinearity and produced a better model than was arrived upon using VIF in (a), which had an R-squared value of 0.957. \textcolor{red}{But the VIF gives values greater than 5? Surely we can't say this?}

Finally, we check the model assumptions, which can be accepted based on the following plots:

```{r, echo=FALSE}
par(mfrow=c(1,2))

qqnorm(residuals(step_up_lm))
plot(fitted(step_up_lm), residuals(step_up_lm))
```

**c)**  
  
Using the step-up model found in (b), the 95% prediction interval for *expend* is given by:

```{r}
new_data <- data.frame(bad=50, crime=5000, lawyers=5000, employ=5000, pop=5000)
predict(step_up_lm, new_data, interval="prediction", level=0.95)
```

We cannot improve this interval since we have already removed the influence points from the data.

```{r, include=FALSE}
predict(step_up_lm, new_data, interval="confidence", level=0.95)
```

**d)**  
  
We can apply the LASSO method as follows:

```{r, include=FALSE}
# install.packages("glmnet", repos = "https://cran.us.r-project.org")
library(glmnet)
```

```{r}
x <- as.matrix(crime_df_upd[, exp_vars])
y <- as.matrix(crime_df_upd[, c(response)])

# train-test splitting
train <- (sample(1:nrow(x), 0.67*nrow(x))) # train by using 2/3 of the data
x.train <- x[train,]; y.train <- y[train]
x.test <- x[-train,]; y.test <- y[-train]

# fitting the model
lasso.mod <- glmnet(x.train, y.train, alpha=1)
cv.lasso <- cv.glmnet(x.train,y.train,alpha=1,type.measure='mse')
```

```{r}
plot(lasso.mod, label=T, xvar="lambda")
```

```{r}
plot(cv.lasso)
```

The resulting choice of lambda is *lambda.1se*, which is given by:

```{r}
(lambda.1se <- cv.lasso$lambda.1se)
```

```{r, include=FALSE}
# https://glmnet.stanford.edu/articles/glmnet.html#assessing-models-on-test-data-1
assess.glmnet(lasso.mod, newx = x.test, newy = y.test, s=cv.lasso$lambda.1se)
coef(lasso.mod, s=lambda.1se) # betaâ€™s for lambda.1se
```

We can evaluate the resulting model by examining the Mean Squared Error (MSE) on the test set.

```{r}
y.pred <- predict(lasso.mod, s=lambda.1se, newx=x.test)
mse.lasso <- mean((y.test - y.pred)^2); mse.lasso
```

To compare this to the step-up model in (b), we can find the MSE for this model on the same test set. \textcolor{red}{Isn't this unfair because the step up model was trained on the entire data set? Do we need to perform step up using only the training dataset? Or maybe we can just point this out but not actually do it?}

```{r}
new_data <- data.frame(x.test)
y.pred <- predict(step_up_lm, new_data, interval="confidence", level=0.95)
mse.step_up <- mean((y.test - y.pred)^2); mse.step_up
```

We see that the step-up model outperforms the LASSO model, producing a smaller MSE. This could be because LASSO is better suited to situations with many more explanatory variables. \textcolor{red}{Or because the step-model used the entire data set?}

## Exercise 3

```{r, include=FALSE}
# Read data
titanic_df <- as.data.frame(read.table("data/titanic.txt", header=TRUE))
head(titanic_df)
```

**a)**  

### Studying the Data  

We can start by looking at the summary data for each column:

```{r}
titanic_df$PClass <- as.factor(titanic_df$PClass)
titanic_df$Sex <- as.factor(titanic_df$Sex)
summary(titanic_df)
```

We see that there were more people in 3rd class than in 1st and 2nd combined. Half the people were between the ages 21 and 39, with an average of 30 years old. Males made up 65% of the passengers, and 34% of all the passengers survived.  

We can also examine how the sexes were distributed over the classes:

```{r}
tot_comb <- xtabs(~PClass+Sex, data=titanic_df)
tot_comb
```

As expected, there are more males in each category, with the biggest difference in 3rd class.  

We can examine the same distribution in terms of percentage of people who survived for each combination: 

```{r}
tot_comb.surv <- xtabs(Survived~PClass+Sex, data=titanic_df)
round(tot_comb.surv/tot_comb, 2)
```

It appears that the survival rate increases from 3rd to 1st class, and is much higher for females in all three cases, although this margin is less pronounced for 3rd class.  

Finally, we can plot the distribution of ages:

```{r}
hist(titanic_df$Age, xlab="Age", main="Age Distribution of Passengers")
```

We can see that the bulk of the ages are grouped around 25, with exponentially fewer people for older groupings.

```{r, include=FALSE}
# I don't think this is plot tells us anything useful?
boxplot(Age ~ Sex + PClass, data=titanic_df, col = c("#FFE0B2", "#F57C00")) 
```

### Fitting the Model  

First, we will removing rows with missing ages:

```{r}
titanic_df_upd <- na.omit(titanic_df)
```

Next, we can fit a logistic regression model and test it as follows:

```{r}
titanic_df_upd$PClass <- as.factor(titanic_df_upd$PClass)
titanic_df_upd$Sex <- as.factor(titanic_df_upd$Sex)
base_lm <- glm(Survived ~ Age+PClass+Sex, data = titanic_df_upd, family = binomial)
```

```{r}
drop1(base_lm, test = "Chisq")
```
We see that all three variables have a significant effect on the odds of survival. To further interpret the results, we can look at the model summary and its coefficients.

```{r}
summary(base_lm)
```

Thus, the estimated odds is:

$$\hat{o}_k=\frac{P \widehat{\left(Y_k=1\right)}}{P \widehat{\left(Y_k=0\right)}} \approx e^{3.76 - 0.04*Age_k - 1.29*PClass2nd_k - 2.52*PClass3rd_k - 2.63*Sexmale_k}$$

We see that all the coefficients are negative. This means that increasing age, being male and moving down in class all lower a passenger's odds of survival. Moreover, the magnitude of the coefficients indicates how sensitive these odds are to each variable. From this we can infer that 3rd class has worse odds than 2nd class, and that being a male is the most influential factor in decreasing one's odds of survival. While increasing age also lowers one's odds, it has the smallest effect compared to to the other two variables.

**b)** 

Investigating the interaction between *Age* and *PClass*:
```{r}
anova(glm(Survived ~ Age*PClass, data = titanic_df_upd, family = binomial), test="Chisq")
```

We see that the interaction is not siginficant. Next, we investigating the interaction between *Age* and *Sex*:

```{r}
anova(glm(Survived ~ Age*Sex, data = titanic_df_upd, family = binomial), test="Chisq")
```

Here, the interaction is significant, and so we wish to include it in our model. *PClass* was significant by itself, so we include it in the final model as follows

\textcolor{red}{<<<<< INVESTIGATE VARIABLES USING anova(smaller model, bigger model) >>>>>>}

```{r}
final_lm <- glm(Survived ~ PClass+Age*Sex, data = titanic_df_upd, family = binomial)
summary(final_lm, test="Chisq")
```

\textcolor{red}{Comments on this result? Is it bad that Age and Sexmale are not siginficant? Maybe just significant as a whole and that's okay?}  

Using this model we can predict the estimate for the probability of survival for each combination of levels of the factors *PClass* and *Sex* for a person of *Age* 55:

```{r}
newdata <- data.frame(Age=c(55, 55, 55, 55, 55, 55), PClass=c("1st", "1st", "2nd", "2nd", "3rd", "3rd"), Sex=c("female", "male", "female", "male", "female", "male"))
predict(final_lm, newdata, type="response")
```

Again we see that being a women appears to be the dominant factor in increased survival, and that the probability of survival also decreases as you go down in class (from 1st to 3rd)

**c)** 

We can predict the survival status of the individuals by looking at the fitted values produced by the model, which gives the probability of survival. We can then set a threshold of 0.5. We can predict that those with a probability lower than this did not survive, and those with a probability higher than this did survive. To measure the quality of this prediction method, we can use tools such as a confusion matrix and log likelihood as quality measures.

**d)** 

**Survived vs Sex**

For 2x2 tables we can obtain exact p-value using the Fisher test as follows:

```{r}
fisher.test(x=titanic_df_upd$Survived, y=titanic_df_upd$Sex)
```

Therefore *Sex* and *Survived* are not independent.  

**Survived vs PClass**

Since we have three classes, we must use the Chi-squared test to investigate the effect of passenger class:

```{r}
chisq.test(x=titanic_df_upd$Survived, y=titanic_df_upd$PClass)
```

Therefore *PClass* and *Survived* are not independent.

**e)** 

No, the approach in (d) is not wrong. Rather there are advantages and disadvantages associated with both methods:

**Logistic Regression**  

*Advantages* - Can handle a combination of continuous and categorical explanatory variables. Gives you detailed information about the role of explanatory variables since it tells you how the variable affects the response (sign of the coefficient), as well as how sensitive the response is to changes in the variable (magnitude of the coefficient).  
*Disadvantages* - https://careerfoundry.com/en/blog/data-analytics/what-is-logistic-regression/  \textcolor{red}{???}

vs  

**Contingency Table**. 

*Advantages* - Works for tables larger than 2x2. Works best for categorical explanatory variables.  
*Disadvantages* - Does not work for continuous variables. Does not provide an exact p-value, only an approximation. Only tells you whether dependence exists, not the nature of the dependence.  

vs  

**Fisher**  

*Advantages* - Provides an exact p-value. Works best for categorical explanatory variables.  
*Disadvantages* - Does not work for continuous variables. Only works for 2x2. Only tells you whether dependence exists, not the nature of the dependence.

## Exercise 4

```{r, include=FALSE}
# Read data
coups_df <- as.data.frame(read.table("data/coups.txt", header=TRUE))
head(coups_df)
```


```{r}
coups_df$pollib <- as.factor(coups_df$pollib)
```


**a)** 
```{r}
poison_glm <- glm(miltcoup ~ oligarchy + parties + pctvote + popn + size + numelec + numregim + pollib, data = coups_df, family = poisson)
drop1(poison_glm, test= "Chisq")
summary(poison_glm)
```


Through Poisson regression, and its summary, we can find that the variables that are significant in predicting number of successful military coups are: *oligarchy*, *pollib*, and *parties*.

**b)** 

```{r, include=FALSE}
step1 <- glm(miltcoup ~ oligarchy + parties + pctvote + popn + size + numelec + pollib, data = coups_df, family = poisson)
drop1(step1, test="Chisq")
```

```{r, include=FALSE}
step2 <- glm(miltcoup ~ oligarchy + parties + pctvote + popn + size + pollib, data = coups_df, family = poisson)
drop1(step2, test="Chisq")
```

```{r, include=FALSE}
step3 <- glm(miltcoup ~ oligarchy + parties + pctvote + popn + pollib, data = coups_df)
drop1(step3, test="Chisq")
```

```{r, include=FALSE}
step4 <- glm(miltcoup ~ oligarchy + parties + popn + pollib, data = coups_df)
drop1(step4, test="Chisq")
```

```{r, include=FALSE}
step5 <- glm(miltcoup ~ oligarchy + parties + pollib, data = coups_df, family = poisson)
drop1(step5, test="Chisq")
```

The step down method was applied, removing the variables: *numelec*, *size*, *pctvote*, and *popn*, respectively. After which, all remaining variables were significant, resulting in the model:
```{r}
final_plm <- glm(miltcoup ~ oligarchy + parties + pollib, data = coups_df, family = poisson)
summary(final_plm, test="Chisq")
```
In comparison with a) all the same variables are significant.

**c)** 

```{r}
coups_df$pollib <- as.factor(coups_df$pollib)
```

```{r}
mean(coups_df$oligarchy); mean(coups_df$parties)
```


```{r}
newdata <- data.frame(pollib=c("0", "1", "2"), oligarchy=c(5.22, 5.22, 5.22), parties=c(17.1, 17.1, 17.1))
predict(final_plm, newdata, type="response")
```

Our model predicts that there will be roughly 3 successful coups for pollib=0, roughly 2 successful coups for pollib=1, and 1 successful coup for pollib=2.
