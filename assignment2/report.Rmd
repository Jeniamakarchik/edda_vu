---
title: "Assignment 2"
author: "Alexia Salomons, Nathan Maxwell Jones, Yauheniya Makarevich, group 71"
date: "15 March 2023"
output: pdf_document
fontsize: 11pt
highlight: tango
---

```{r setup, include = FALSE}
# set up global R options
options(digits = 3)

# set up knitr global chunk options
knitr::opts_chunk$set(fig.height = 3)
```


## Exercise 1

```{r, include=FALSE}
tree_df <- as.data.frame(read.table("data/treeVolume.txt", header=TRUE))
head(tree_df)
```

**a)** 
To investigate whether tree type influences total wood volume, we can perform a one-way ANOVA.
```{r}
tree_df$type <- as.factor(tree_df$type)
tree_type_lm <- lm(volume~type, data=tree_df)
anova(tree_type_lm)
```
```{r}
summary(tree_type_lm)
```
With $p > 0.05$, we can conclude that *type* does not have a significant effect on *volume*. Because the factor *type* has two levels, we can apply a two sample t-test.
```{r}
mask <- tree_df$type == "beech"
t.test(tree_df$volume[mask], tree_df$volume[!mask])
```
This supports the result from the ANOVA test. The estimated volume is 30.2 for Beech trees and 35.2 for Oak trees.

**b)** 
To investigate this claim, we create two models, each including all three explanatory variables (*type*, *diameter* and *height*). In the first model, we also include the pairwise interaction between *type* and *diameter*.
```{r}
tree_type_d_lm <- lm(volume~height+type*diameter, data=tree_df)
anova(tree_type_d_lm)
```

```{r}
summary(tree_type_d_lm)
```


```{r}
tree_type_h_lm <- lm(volume~diameter+type*height, data=tree_df)
anova(tree_type_h_lm)
```

```{r}
summary(tree_type_h_lm)
```

We see that both pairwise interactions are not significant. Therefore, we can conclude that both *height* and *diameter* have the same influence regardless of *type*. Both models suggest that all three explanatory variables have a significant effect individually.

**c)** 

In (b), we saw that the interactions of *height* and *diameter* with *type* were not significant, and so we will investigate a purely additive model (assuming no interactions).

```{r}
tree_add_all_lm <- lm(volume~diameter+height+type, data=tree_df)
anova(tree_add_all_lm)
```
```{r}
summary(tree_add_all_lm)
```

We see that the effect of *type* is not significant in the additive model. Therefore we will investigate an additive model that excludes *type*.

```{r}
tree_add_dh_lm <- lm(volume~diameter+height, data=tree_df)
anova(tree_add_dh_lm)
```
```{r}
summary(tree_add_dh_lm)
```
This model has almost the same R-squared value as before, while using fewer variables. Since simpler models are generally preferred, this is our model of choice to make predictions. As a final test, we need to check this model's assumptions to ensure that the conclusions we draw from it are valid:

```{r, echo=FALSE}
par(mfrow=c(1, 2))
qqnorm(residuals(tree_add_dh_lm))
plot(fitted(tree_add_dh_lm), residuals(tree_add_dh_lm))
```
While these plots are not perfect, we believe the model assumptions to be valid. 

Therefore, the effects of *type*, *diameter* and *height* can be summarized as follows:

* The tree *type* does not affect volume significantly.
* Looking at the coefficients, we see that increasing both height and diameter result in an increase in volume, with diameter having a bigger impact (with a gradient of 4.63 compared to *height's* 0.43). This makes sense given that we know volume is proportional to the the square of the diameter.

To predict the volume for a tree with the overall average diameter and height, we can use the following linear regression model:

$$volume = -64.37 + 4.63 * diameter + 0.43 * height$$
```{r}
mean_d <- mean(tree_df$diameter)
mean_h <- mean(tree_df$height)
means <-  data.frame(diameter=c(mean_d), height=c(mean_h))

predict(tree_add_dh_lm, means, se.fit = TRUE)
```
Therefore we expect the volume for such a tree to be 32.6.

**d)** 
Assuming that a tree is roughly cylindrical, we expect that *volume* would be proportional to the *height*, multiplied by the square of *diameter*. We perform this transformation and add it as a new column in the data frame. We could apply the true transformation, $V = h \times \pi (d/2)^2$, but this would just add unnecessary constants which would already be captured in the regression coefficients.
```{r}
tree_df$math_volume <- tree_df$height * tree_df$diameter^2
math_volume_lm <- lm(volume~math_volume, data=tree_df)
anova(math_volume_lm)
```

```{r}
summary(math_volume_lm)
```
We see that this transformation does indeed produce an explanatory value with significant effect. We also see that the R-squared value of 0.975 is higher than that of the previous models, indicating that it better explains the data. Finally, we check the assumptions of this model.
```{r, echo=FALSE}
par(mfrow=c(1,2))

qqnorm(residuals(math_volume_lm))
plot(fitted(math_volume_lm), residuals(math_volume_lm))
```

These plots are acceptable, meaning we can accept the model assumptions.

## Exercise 2

```{r, include=FALSE}
# Read data
crime_df <- as.data.frame(read.table("data/expensescrime.txt", header=TRUE))
head(crime_df)

target <- "expend"
columns <- c("bad", "crime", "lawyers", "employ", "pop")
```

**a)** 

Let's find outliers and influence points and then check for collinearity.

```{r}
model <- lm(expend~bad+crime+lawyers+employ+pop, data=crime_df)
cooks.distance(model)[cooks.distance(model) > 1]
```
```{r, echo=FALSE}
plot(cooks.distance(model), type='b')
```

```{r}
round(cor(crime_df[, c(columns, target)]))
```

```{r}
pairs(cor(crime_df[, c(columns, target)]))
```
```{r}
outliers <- crime_df[c(5,8,35,44),]
outliers
```

```{r}
# removing outliers from the dataframe
crime_df_upd <- crime_df[c(-5,-8,-35,-44),]
crime_df_upd
```
```{r}
model <- lm(expend~bad+crime+lawyers+employ+pop, data=crime_df_upd)
cooks.distance(model)[cooks.distance(model) > 1]
```
```{r}
plot(cooks.distance(model), type='b')
```

```{r}
pairs(cor(crime_df_upd[, c(columns, target)]))
```

```{r, include=FALSE}
library(car)
```

```{r}
vif(model)
```

```{r}
model <- lm(expend~bad+crime+lawyers+pop, data=crime_df_upd)
vif(model)
```
```{r}
model <- lm(expend~bad+crime+lawyers, data=crime_df_upd)
vif(model)
```

```{r}
pairs(cor(crime_df_upd[, c("bad", "crime", "lawyers", target)]))
```

**b)** 

```{r}
# 1. empty model
summary(lm(expend~1, data=crime_df_upd))
```

```{r}
# 1. empty model
summary(lm(expend~1, data=crime_df_upd))
```

```{r}
columns
```


```{r}
# 2. empty model
summary(lm(expend~bad, data=crime_df_upd))
```

```{r}
# 2. empty model
summary(lm(expend~crime, data=crime_df_upd))
```

```{r}
# 2. empty model
summary(lm(expend~lawyers, data=crime_df_upd))
```

```{r}
# 2. empty model
summary(lm(expend~employ, data=crime_df_upd))
```

```{r}
# 2. empty model
summary(lm(expend~pop, data=crime_df_upd))
```
```{r}
# 3.two var model
summary(lm(expend~employ+bad, data=crime_df_upd))
```

```{r}
# 3.two var model
summary(lm(expend~employ+crime, data=crime_df_upd))
```

```{r}
# 3.two var model
summary(lm(expend~employ+lawyers, data=crime_df_upd))
```

```{r}
# 3.two var model
summary(lm(expend~employ+pop, data=crime_df_upd))
```

```{r}
# 4. three var model
summary(lm(expend~employ+crime+bad, data=crime_df_upd))
```

```{r}
# 4. three var model
summary(lm(expend~employ+crime+lawyers, data=crime_df_upd))
```

```{r}
# 4. three var model
summary(lm(expend~employ+crime+pop, data=crime_df_upd))
```

```{r}
# 5. four var model
summary(lm(expend~employ+crime+pop+bad, data=crime_df_upd))
```

```{r}
# 5. four var model
summary(lm(expend~employ+crime+pop+lawyers, data=crime_df_upd))
```

Final model: expend = -247 + 0.0209\*employ + 0.0543\*crime + 0.0714\*pop $\pm$ error, with $R^2 = 0.974$.
Step - up naturally removes collinearity and can be compared with VIF results.

```{r}
model <- lm(expend~employ+crime+pop, data=crime_df_upd)
summary(model)
```


```{r, echo=FALSE}
par(mfrow=c(1,2))

qqnorm(residuals(model))
plot(fitted(model), residuals(model))
```

**c)** 
```{r}
mean(crime_df_upd$expend)
```

```{r}
new_data <- data.frame(bad=50, crime=5000, lawyers=5000, employ=5000, pop=5000)
predict(model, new_data, interval="prediction", level=0.95)
```

If we thinl that improvement means making interval smaller, we can go for confidence interval.

```{r}
predict(model, new_data, interval="confidence", level=0.95)
```

**d)** 

```{r}

```

## Exercise 3

```{r, include=FALSE}
# Read data
titanic_df <- as.data.frame(read.table("data/titanic.txt", header=TRUE))
```

```{r}
head(titanic_df)
```

```{r}
plot(titanic_df)
```


**a)** 

```{r}
titanic_df_upd <- na.omit(titanic_df)
titanic_df_upd$PClass <- as.factor(titanic_df_upd$PClass)
titanic_df_upd$Sex <- as.factor(titanic_df_upd$Sex)
head(titanic_df_upd)
```

**b)** 

```{r}

```

**c)** 

```{r}

```

**d)** 

```{r}

```

**e)** 

```{r}

```

## Exercise 4

```{r, include=FALSE}
# Read data
```

**a)** 

```{r}

```

**b)** 

```{r}

```

**c)** 

```{r}

```

